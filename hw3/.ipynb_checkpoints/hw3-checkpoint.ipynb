{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkqdwZcO97qw"
   },
   "source": [
    "# Exercise 3: MAP Classifier\n",
    "\n",
    "In this assignment you will implement Bayesian learning\n",
    "\n",
    "## Do not start the exercise until you fully understand the submission guidelines.\n",
    "\n",
    "* The homework assignments are executed automatically. \n",
    "* Failure to comply with the following instructions will result in a significant penalty. \n",
    "* Appeals regarding your failure to read these instructions will be denied. \n",
    "* Kindly reminder: the homework assignments contribute 50% of the final grade.\n",
    "\n",
    "\n",
    "## Read the following instructions carefully:\n",
    "\n",
    "1. This Jupyter notebook contains all the step-by-step instructions needed for this exercise.\n",
    "1. Write **efficient**, **vectorized** code whenever possible. Some calculations in this exercise may take several minutes when implemented efficiently, and might take much longer otherwise. Unnecessary loops will result in point deductions.\n",
    "1. You are responsible for the correctness of your code and should add as many tests as you see fit to this jupyter notebook. Tests will not be graded nor checked.\n",
    "1. Complete the required functions in `hw3.py` script only. This exercise is graded automatically, and only the `hw3.py` script is tested.\n",
    "1. You are allowed to use functions and methods from the [Python Standard Library](https://docs.python.org/3/library/), numpy and pandas only. **Do not import anything else.**\n",
    "1. Your code must run without errors. Use at least `numpy` 1.15.4. Any code that cannot run will not be graded.\n",
    "1. Write your own code. Cheating will not be tolerated.\n",
    "1. Submission includes this notebook with the exercise number and your ID as the filename and the `hw3.py` script. For example: `hw3_123456789_987654321.ipynb` and `hw3.py` if you submitted in pairs and `hw3_123456789.ipynb` and `hw3.py` if you submitted the exercise alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 596,
     "status": "ok",
     "timestamp": 1649263726391,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "S7n52AXs97q6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make the notebook automatically reload external python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wIlV22zUVJ7p"
   },
   "source": [
    "# Conditional independence  \n",
    "Define 3 random variables (RV) $X, Y, C$ s.t.:  \n",
    "1. $X, Y$ and $C$ are all binary\n",
    "2. The following conditions hold:  \n",
    "    * P(X=0) = 0.3\n",
    "    * P(Y=0) = 0.3\n",
    "    * P(C=0) = 0.5\n",
    "3. $X$ and $Y$ are not independent\n",
    "4. $X$ and $Y$ are conditionally independent given $C$ $(X \\perp\\!\\!\\!\\perp Y |C)$\n",
    "\n",
    "In order to define those RV you need to fill the distributions (represent as python dictionaries) below\n",
    "and then write a function that prove that conditions 3 (`is_X_Y_dependent`) and 4 (`is_X_Y_given_C_independent`) holds.\n",
    "\n",
    "Recall that:   \n",
    "1. $P(X|Y) = \\frac{P(X, Y)}{P(Y)}$  \n",
    "2. $P(X, Y|C) = \\frac{P(X, Y, C)}{P(C)}$\n",
    "3. $(X \\perp\\!\\!\\!\\perp Y |C)$   iff  \n",
    "$\\forall x, y,c$: $p(X=x,Y=y|C=c)=p(X=x|C=c)p(Y=y|C=c)$  \n",
    "\n",
    "**Make sure that all the probabilities are valid!**\n",
    "\n",
    "You may assume that X, Y and C have the same support (e.g. they are defined on the same space).\n",
    "\n",
    "Note: since python suffer from numerical instability you may want to use `np.isclose` instead of the `==` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw3 import conditional_independence\n",
    "\n",
    "print(conditional_independence().is_X_Y_dependent())\n",
    "print(conditional_independence().is_X_Y_given_C_independent())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZJBM6CCAyRB"
   },
   "source": [
    "# Maximum Likelihood estimation  \n",
    "\n",
    "In probability theory and statistics, the Poisson distribution is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant mean rate and independently of the time since the last event.  \n",
    "The parameter of this distribution is the rate of events in that given time interval, annotated $\\lambda$  \n",
    "if $X$~$Pois(\\lambda)$  \n",
    "then $p(X=k|\\lambda) = \\frac{\\lambda^ke^{-\\lambda}}{k!}$  \n",
    "Where $X$ is a RV $\\lambda$ is the rate and $p$ is the pmf\n",
    "\n",
    "Implement the function `poisson_log_pmf` in `hw3.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ae0FUmlqFPA1"
   },
   "outputs": [],
   "source": [
    "from hw3 import poisson_log_pmf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weRAiHJxIbuh"
   },
   "source": [
    "In the file poisson_1000_samples.csv there are 1000 points drawn from some poisson distribution with a fixed parameter $\\lambda$  \n",
    "\n",
    "In the following section you are going to find a rate that maximizes the likelihood function. You will do this in 2 different ways:\n",
    "1. Iterative (`possion_iterative_mle`): given a list of possible rates (`rates`), calculate the log likelihood value for each rate and return the rate that has the maximum value\n",
    "2. Analytic (`possion_analytic_mle`): read the following blog: https://www.statology.org/mle-poisson-distribution/. This blog demonstrate how to derive the MLE of a poisson distribution. Understand the process and implement the function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "executionInfo": {
     "elapsed": 311,
     "status": "error",
     "timestamp": 1649264049923,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "14_MylZP-15d",
    "outputId": "014abf33-e03c-4ef7-b787-eb1a00d0815b"
   },
   "outputs": [],
   "source": [
    "poisson_samples = pd.read_csv('data/poisson_1000_samples.csv').values.flatten()\n",
    "rates = np.linspace(0.01, 20, num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 295,
     "status": "ok",
     "timestamp": 1649264030874,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "Z8C_qIhKA9ZK"
   },
   "outputs": [],
   "source": [
    "from hw3 import (get_poisson_log_likelihoods, \n",
    "                    possion_iterative_mle,\n",
    "                    possion_analytic_mle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "executionInfo": {
     "elapsed": 779,
     "status": "error",
     "timestamp": 1649264034100,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "i55OIUH1MIUh",
    "outputId": "b593730f-e29a-4ba0-fd56-0f2a8bc4f29d"
   },
   "outputs": [],
   "source": [
    "x = rates\n",
    "y = get_poisson_log_likelihoods(poisson_samples, rates)\n",
    "iterative_rate = possion_iterative_mle(poisson_samples, rates)\n",
    "analytic_rate = possion_analytic_mle(poisson_samples)\n",
    "\n",
    "plt.plot(x, y)\n",
    "plt.axvline(x=iterative_rate, linestyle=':', c='r', label=f\"iterative: {iterative_rate:.2f}\")\n",
    "plt.axvline(x=analytic_rate, linestyle='--', c='g', label=f\"analytic: {analytic_rate:.2f}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kL1Y_nmu97q9"
   },
   "source": [
    "\n",
    "# Normal Naive Bayes Classifier Vs Normal Full Bayes Classifier\n",
    "In the following section we are going to compare 2 models on a given dataset. <br>\n",
    "The 2 classifiers we are going to test are:\n",
    "1. Naive Bayes classifier.<br>\n",
    "1. Full Bayes classifier.<br>\n",
    "Recall that a Naive Bayes classifier makes the following assumption :<br> \n",
    "## $$ p(x_1, x_2, ..., x_n|A_j) = \\Pi p(x_i | A_j) $$\n",
    "But the full Bayes classifier will not make this assumption.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O4Trl8uU97q-"
   },
   "source": [
    "### The Data Story"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Z8oRyIG97rA"
   },
   "source": [
    "In a faraway land called **Randomistan** there is a rare animal called the **Randomammal**.<br> \n",
    "We have gathered data about this unique animal to help the **randomian** researchers in observing this beast. <br>\n",
    "For a 1000 days straight we have measured the temperature and the humidity in Randomistan and whether the Randomammal was spotted or not. <br>\n",
    "The well known randomian **Bob** is a bit of a lazy researcher so he likes to keep things simple, and so he assumes that the temperature and the humidity are independent given the class. <br>\n",
    "**Alice** on the other hand is a hard working researcher and does not make any assumptions, she's young and is trying to gain some fame in the randomian community.\n",
    "\n",
    "The dataset contains 2 features (**Temperature**, **Humidity**) alongside a binary label (**Spotted**) for each instance.<br>\n",
    "\n",
    "We are going to test 2 different classifiers :\n",
    "* Naive Bayes Classifier (Bob)\n",
    "* Full Bayes Classifier. (Alice)\n",
    "\n",
    "Both of our researchers assume that our features are normally distributed. But while Bob with his Naive classifier will assume that the features are independent, Alice and her Full Bayes classifier will not make this assumption.<br><br>\n",
    "Let's start off by loading the data (train, test) into a pandas dataframe and then converting them\n",
    "into numpy arrays.<br>\n",
    "The datafiles are :\n",
    "- randomammal_train.csv\n",
    "- randomammal_test.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wml9l2VJ97rB"
   },
   "outputs": [],
   "source": [
    "# Load the train and test set into a pandas dataframe and convert them into a numpy array.\n",
    "# The columns order: ['Temp', 'Humidity', 'Spotted']\n",
    "train_set = pd.read_csv('data/randomammal_train.csv').values\n",
    "test_set = pd.read_csv('data/randomammal_test.csv').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0Smujya97rD"
   },
   "source": [
    "# Data Visualization\n",
    "Note the scatter plot of the training data where __x__=Temperature and **y**=Humidity. <br>\n",
    "Color is used to distinguish points from different classes.<br>\n",
    "Stop for a minute to think about Alice and Bob's approaches and which one you expect to work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e3GuXpOj97rF"
   },
   "outputs": [],
   "source": [
    "plt.scatter(x=train_set[:, 0], y=train_set[:, 1], c=train_set[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mtalzG-N97rG"
   },
   "source": [
    "## Bob's Naive Model\n",
    "\n",
    "Start with implementing the [normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) probability density function in `hw3.py`: \n",
    "$$ \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\cdot e ^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} $$\n",
    "Where :\n",
    "* $\\mu$ is the distribution mean.\n",
    "* $\\sigma$ is the distribution standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw3 import normal_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zLQsBEja97rH"
   },
   "source": [
    "Implement the **NaiveNormalClassDistribution** in `hw3.py` and build a distribution object for each class.\n",
    "Recall that when using the naive assumption, we assume our features are independent given the class. Meaning:\n",
    "$$ P(x_1, x_2 | Y) = p(x_1 | Y) \\cdot p(x_2 | Y)$$\n",
    "\n",
    "\n",
    "Since we assume our features are normally distributed we need to find the mean and std for each feature in order for us to compute those probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BSJRUYiZ97rI"
   },
   "outputs": [],
   "source": [
    "from hw3 import NaiveNormalClassDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RDlv-Qu897rK"
   },
   "outputs": [],
   "source": [
    "# Build the a NaiveNormalClassDistribution for each class.\n",
    "naive_normal_CD_0 = NaiveNormalClassDistribution(train_set, 0)\n",
    "naive_normal_CD_1 = NaiveNormalClassDistribution(train_set, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3S8TaXg97rK"
   },
   "source": [
    "Implement the **MAPClassifier** class and build a MAPClassifier object containing the 2 distribution objects you just made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw3 import MAPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQfcw22U97rL"
   },
   "outputs": [],
   "source": [
    "naive_normal_classifier = MAPClassifier(naive_normal_CD_0, naive_normal_CD_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fK3PKXpf97rM"
   },
   "source": [
    "### Evaluate model\n",
    "Implement the **compute_accuracy** function in `hw3.py`. Use it and the 2 distribution objects you created to compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw3 import compute_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1649236759557,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "X-p0Oo2A97rM",
    "outputId": "6ad81ff3-d37a-406a-83dd-0d061ffed43e"
   },
   "outputs": [],
   "source": [
    "# Compute the naive model accuracy and store it in the naive accuracy variable.\n",
    "naive_accuracy = compute_accuracy(test_set, naive_normal_classifier)\n",
    "naive_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_mGpmGM97rN"
   },
   "source": [
    "## Alice's Full Model\n",
    "\n",
    "Start with Implementing the [multivariate normal](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) distribution probability density function in `hw3.py`.\n",
    "\n",
    "## $$ (2\\pi)^{-\\frac{d}{2}} det(\\Sigma )^{-\\frac{1}{2}} \\cdot e ^{-\\frac{1}{2}(x-\\mu)^T \\Sigma ^ {-1} (x - \\mu) }$$\n",
    "\n",
    "Where : \n",
    "* $\\mu$ is the distribution mean vector. (length 2 in our case)\n",
    "* $\\Sigma$ Is the distribution covariance matrix. (size 2x2 in our case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bZTlpgi7Ojal"
   },
   "outputs": [],
   "source": [
    "from hw3 import multi_normal_pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oIo8Ji8Z97rN"
   },
   "source": [
    "Implement the **MultiNormalClassDistribution** and build a distribution object for each class.\n",
    "\n",
    "In the full bayes model we will not make any simplifying assumptions, meaning, we will use a multivariate normal distribution. <br>\n",
    "And so, we'll need to compute the mean of each feature and to compute the covariance between the features to build the covariance matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJDo4AeC97rO"
   },
   "outputs": [],
   "source": [
    "from hw3 import MultiNormalClassDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yRxCmrfI97rO"
   },
   "outputs": [],
   "source": [
    "# Build the a MultiNormalClassDistribution for each class.\n",
    "multi_normal_CD_0 = MultiNormalClassDistribution(train_set, 0)\n",
    "multi_normal_CD_1 = MultiNormalClassDistribution(train_set, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wMYfYFiF97rO"
   },
   "source": [
    "build a MAPClassifier object containing the 2 distribution objects you just made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0dM1VNHn97rP"
   },
   "outputs": [],
   "source": [
    "multi_normal_classifier = MAPClassifier(multi_normal_CD_0, multi_normal_CD_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fmTzieM97rP"
   },
   "source": [
    "### Evaluate model\n",
    "Use the **compute_accuracy** function and the 2 distribution objects you created to compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 401,
     "status": "ok",
     "timestamp": 1649238247985,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "9Ihutafq97rP",
    "outputId": "9ba3c303-f2e1-44b5-f780-df7896cc5584"
   },
   "outputs": [],
   "source": [
    "# Compute the naive model accuracy and store it in the naive accuracy variable.\n",
    "full_accuracy = compute_accuracy(test_set, multi_normal_classifier)\n",
    "full_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_7u-ec397rQ"
   },
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_XxF0vm797rQ"
   },
   "source": [
    "Use a plot bar to showcase the models accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "executionInfo": {
     "elapsed": 393,
     "status": "ok",
     "timestamp": 1649238258489,
     "user": {
      "displayName": "Yarden Rachamim",
      "userId": "05474227465087296318"
     },
     "user_tz": -180
    },
    "id": "tCi0JFha97rQ",
    "outputId": "d1a6baa6-9149-40e9-af8b-52caff47a5df"
   },
   "outputs": [],
   "source": [
    "# Bar plot of accuracy of each model side by side.\n",
    "plt.bar(x=['Naive', 'Full'], height=[naive_accuracy, full_accuracy])\n",
    "plt.title(\"Naive vs Full accuracy comparison\")\n",
    "plt.ylabel(\"Accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUxx4QO697rR"
   },
   "source": [
    "# Comparing Max a posteriori, prior, and likelihood results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JvoTqYI397rR"
   },
   "source": [
    "For each of the classifiers above (naive Bayes and full Bayes, in which we compare posterior probabilities), we explore how classifiers would perform if we compare (1) only prior probabilities or (2) only likelihoods. \n",
    "\n",
    "In this section, you will implement MaxPrior and MaxLikelihood classifiers similarly to MAPClassifier, and then visualize the performance of the three models (MAP, MaxPrior, and MaxLikelihood) for each of the examples of above (naive Bayes and full Bayes).\n",
    "\n",
    "For example, your visualization can be a graph where accuracy is the y-axis, \"MaxPrior\", \"MaxLikelihood\", and \"MAP\" are the x-axis values, and at each x-value, there will be two bars - one for the naive Bayes, and one for the full Bayes.  \n",
    "\n",
    "Other graphs (that make sense / are intuitive) will be accepted as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpIKZphj97rS"
   },
   "source": [
    "Implement the **MaxPrior** class and build a MaxPrior object like you did above with the **MAPClassifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw3 import MaxPrior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jOncByj97rS"
   },
   "source": [
    "Implement the **MaxLikelihood** class and build a MaxLikelihood object like you did above with the **MAPClassifier**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uk7culTl97rT"
   },
   "outputs": [],
   "source": [
    "from hw3 import MaxLikelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2r0piw097rT"
   },
   "source": [
    "### Run and evaluate the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KpKqIqTy97rU"
   },
   "source": [
    "Repeat the process you did for the MAPClassifier, now for the MaxPrior and MaxLikelihood classifiers:\n",
    "1. Feed the naive_normal distributions and the multi_normal distributions you made for each class into the new models you made in this section\n",
    "2. Evaluate the accuracies\n",
    "3. Plot the results as described in the beginning of this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o7zQccmN97rV"
   },
   "outputs": [],
   "source": [
    "cs = [MaxPrior, MaxLikelihood, MAPClassifier]\n",
    "fig, ax = plt.subplots(len(cs), figsize=(15,10))\n",
    "for i, c in enumerate(cs):\n",
    "    naive_normal_classifier = c(naive_normal_CD_0, naive_normal_CD_1)\n",
    "    multi_normal_classifier = c(multi_normal_CD_0, multi_normal_CD_1)\n",
    "\n",
    "    naive_accuracy = compute_accuracy(test_set, naive_normal_classifier)\n",
    "    full_accuracy = compute_accuracy(test_set, multi_normal_classifier)\n",
    "\n",
    "    # Bar plot of accuracy of each model side by side.\n",
    "#     ax[i].bar(x=['Naive', 'Full'], height=[naive_accuracy, full_accuracy], )\n",
    "    ax[i].set_title(\"{} - Naive vs Full accuracy comparison\".format(c.__name__))\n",
    "    bars = ax[i].bar(['Naive', 'Full'], [naive_accuracy, full_accuracy])\n",
    "    ax[i].bar_label(bars)\n",
    "    ax[i].set_ylim(0,1)\n",
    "\n",
    "    # ax[i].ylabel(\"Accuracy\")\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2TWhBD4997rV"
   },
   "source": [
    "# Discrete Naive Bayes Classifier "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKakgRD797rV"
   },
   "source": [
    "We will now build a discrete naive Bayes based classifier using **Laplace** smoothing.\n",
    "In the recitation, we saw how to compute the probability for each attribute value under each class:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNv3VdNY97rW"
   },
   "source": [
    "$$ P(x_j | A_i) = \\frac{n_{ij} + 1}{n_i + |V_j|} $$\n",
    "Where:\n",
    "* $n_{ij}$ The number of training instances with the class $A_i$ and the value $x_j$ in the relevant attribute.\n",
    "* $n_i$ The number of training instances with the class $A_i$\n",
    "* $|V_j|$ The number of possible values of the relevant attribute.\n",
    "\n",
    "In order to compute the likelihood we assume:\n",
    "$$ P(x| A_i) = \\prod\\limits_{j=1}^{n}P(x_j|A_i) $$\n",
    "\n",
    "And to classify an instance we will choose : \n",
    "$$\\arg\\!\\max\\limits_{i} P(A_i) \\cdot P(x | A_i)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95Y9WEKO97rW"
   },
   "source": [
    "## Data\n",
    "We will try to predict breast cancer again only this time from a different dataset, \n",
    "<br> you can read about the dataset here : [Breast Cancer Dataset](https://archive.ics.uci.edu/ml/datasets/breast+cancer)<br>\n",
    "Load the training set and test set provided for you in the data folder.\n",
    " - breast_trainset.csv\n",
    " - breast_testset.csv\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ldj_5a3d97rX"
   },
   "outputs": [],
   "source": [
    "# Load the train and test set into a pandas dataframe and convert them into a numpy array.\n",
    "train_set = pd.read_csv('data/breast_trainset.csv').values\n",
    "test_set = pd.read_csv('data/breast_testset.csv').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXzx4U0097rX"
   },
   "source": [
    "## Build A Discrete Naive Bayes Distribution for each class\n",
    "Implement the **DiscreteNBClassDistribution** in `hw3.py` and build a distribution object for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hS-DkveU97rX"
   },
   "outputs": [],
   "source": [
    "from hw3 import DiscreteNBClassDistribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uiJVXw5h97rY"
   },
   "outputs": [],
   "source": [
    "discrete_naive_CD_0 = DiscreteNBClassDistribution(train_set, 0)\n",
    "discrete_naive_CD_1 = DiscreteNBClassDistribution(train_set, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c7Zfpdyt97rY"
   },
   "source": [
    "build a MAPClassifier object containing the 2 distribution objects you just made above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hw3 import MAPClassifier_DNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R46vXMqS97rZ"
   },
   "outputs": [],
   "source": [
    "discrete_naive_classifier = MAPClassifier_DNB(discrete_naive_CD_0, discrete_naive_CD_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bKtJw1Ty97rZ"
   },
   "source": [
    "Use the compute_accuracy **method** and the 2 distribution objects you created to compute the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mxwH3d4Z97rZ"
   },
   "outputs": [],
   "source": [
    "discrete_naive_classifier.compute_accuracy(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ProbabilisticModels.ipynb",
   "provenance": [
    {
     "file_id": "1bCEDw-NC2JWZstuBhGlo7VcB188Ft2K5",
     "timestamp": 1649263512426
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
